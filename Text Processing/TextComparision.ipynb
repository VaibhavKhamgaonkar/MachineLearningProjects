{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Product\\Anaconda3_Installed\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np,string\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a class to handle all the Feature enginnering task\n",
    "\n",
    "class Process():\n",
    "    def __init__(self):\n",
    "        self.stemmer = nltk.stem.porter.PorterStemmer()\n",
    "        #self.text = []\n",
    "        self.dictionary = None\n",
    "        self.bow = None\n",
    "        self.tfidf = None\n",
    "        self.sim = None\n",
    "    \n",
    "    # performing the cleaning operation on the text to get only unique words.\n",
    "    def clean_text(self,text):\n",
    "        noPunc = [char for char in text if char not in string.punctuation] # removing the punctuations\n",
    "        noPunc = ''.join(noPunc)\n",
    "        noPunc = [word.lower() for word in noPunc.split() if word.lower() not in stopwords.words('english')]# removing the stopwords\n",
    "        #print(noPunc)\n",
    "        noPunc = [self.stemmer.stem(word) for word in noPunc] # stemming on each word\n",
    "        #noPunc = ' '.join(noPunc)\n",
    "        return noPunc \n",
    "    \n",
    "    #Function to Perform Text Analysis\n",
    "    \n",
    "    def text_analysis(self,text):\n",
    "        #creating a dictionary of all the words\n",
    "        self.dictionary = gensim.corpora.Dictionary(text)\n",
    "        #Creating a Bag of word count for each unique words\n",
    "        self.bow = [self.dictionary.doc2bow(word) for word in text]\n",
    "        #Creating TFiDF for each word\n",
    "        self.tfidf = gensim.models.TfidfModel(self.bow)\n",
    "        \n",
    "    #creating a Similarity checking function\n",
    "    \n",
    "    def check_Similarity(self, text):\n",
    "        self.sim = gensim.similarities.Similarity(output_prefix='similarText',corpus=self.tfidf[self.bow],num_features=len(self.dictionary),num_best=3)\n",
    "        text = self.clean_text(text)\n",
    "        text = self.dictionary.doc2bow(text)\n",
    "        text = self.tfidf[text]\n",
    "        \n",
    "        #checking the similarities        \n",
    "        return self.sim[text]\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "del p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Process()\n",
    "text = ['LAPTOPS are Good by the ways',\n",
    "        'movie is awesome.',\n",
    "        'this mobile is beautiful.',\n",
    "        'how to remove virus',\n",
    "       'mangos are awesome.']\n",
    "\n",
    "temp = []\n",
    "for i in text:\n",
    "    temp.append(p.clean_text(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['laptop', 'good', 'way'],\n",
       " ['movi', 'awesom'],\n",
       " ['mobil', 'beauti'],\n",
       " ['remov', 'viru'],\n",
       " ['mango', 'awesom']]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.text_analysis(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(3, 1), (4, 1)],\n",
       " [(5, 1), (6, 1)],\n",
       " [(7, 1), (8, 1)],\n",
       " [(4, 1), (9, 1)]]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel(num_docs=5, num_nnz=11)\n"
     ]
    }
   ],
   "source": [
    "print(p.tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 1.0), (1, 0.24478666484355927)]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.check_Similarity('fruits like mangos are awesome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
